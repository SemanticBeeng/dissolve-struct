\documentclass[twoside,11pt]{article}


% JMLR Machine Learning Open Source Software papers:
% those are 4 pages plus references
% more details:
% http://www.jmlr.org/mloss/mloss-info.html


% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{bm,color}
\usepackage{mathdots}
\usepackage{subcaption}
\usepackage{amsopn}

\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkblue
}

% Definitions of handy macros can go here

\newcommand{\algname}{\textsc{Dissolve}$\,^{\textsf{\tiny struct}}$\xspace}
\newcommand{\svmstruct}{\textsc{SVM}$\,^{\textsf{\tiny struct}}$\xspace}
\newcommand{\cocoa}{\textsc{CoCoA}\xspace}
\newcommand{\spark}{\textsc{Spark}\xspace}
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|} 

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newcommand{\w}{{\bf w}}

\DeclareMathOperator*{\argmax}{arg\,max}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2015}{1-??}{1/15}{1/15}{Tribhuvanesh Orekondy, Martin Jaggi and Aurelien Lucchi}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Orekondy, Jaggi and Lucchi}
\firstpageno{1}

\begin{document}

\title{\algname -- A Library for Distributed Structured Prediction}

\author{%
       \name Tribhuvanesh Orekondy \email torekond@student.ethz.ch  \\
       \addr ETH Zurich
       \AND
       \name Martin Jaggi \email jaggi@inf.ethz.ch \\
       \addr ETH Zurich
       \AND
       \name Aurelien Lucchi \email aurelien.lucchi@inf.ethz.ch  \\
       \addr ETH Zurich
       }
       

\editor{t.b.d.}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes \algname, a modular and flexible
open source software package for distributed training of structured
prediction models, such as structured SVMs. 
Project website: \href{http://github.com/dalab/dissolve}{github.com/dalab/dissolve}.

We support a broad range of applications, and interfaces to scala, java and python. Our framework is empowered by the fault tolerant \spark computing platform, and automatically adopts to the existing tradeoffs of computation vs communication cost on real world systems. 
The proposed distributed algorithm combines the recent communication efficient \cocoa scheme \citep{Jaggi:2014vi} with the state of the art primal-dual structured prediction solvers \citep{LacosteJulien:2013ue}, and improves further by adding some new ideas for caching oracle answers. 
The framework allows approximate inference, and provides a similar standard interface as \svmstruct for the user.
\end{abstract}

\begin{keywords}
  Structured Prediction, Structured SVM, Distributed Training
\end{keywords}

\section{Introduction}

Structured prediction has gained a lot of popularity over the past few years due to its ability to process structured objects such as images or text documents.

The structured support vector machine (SSVM) is a particularly successful variant of this approach that can be optimized in various ways, including a cutting-plane, stochastic gradient descent or a primal-dual scheme such as MARTIN's paper. The latter is especially appropriate for large-scale problems ...

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

\section{Structured SVM formulation}

A structured model predicts the labeling $Y$ for a given input $X$ by maximizing some score function
$S_\mathbf{w}:\mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$,
i.e.,
%
\begin{equation}
\label{eq:inference}
\hat{Y} = \argmax_{Y \in \mathcal{Y}} S_\w(Y) = \argmax_{Y \in \mathcal{Y}} \w^T\Psi(X,Y),
\end{equation}
%
where $\w$ are the model parameters and $\Psi(X,Y)$ is the \emph{feature map} corresponding to the input $X$ and the labeling $Y$.


Given a set of $N$ training examples $\mathcal{D}=((X^1,Y^1), \dots, (X^N,Y^N))$ where $X^i \in \mathcal{X}$ and $Y^i \in \mathcal{Y}$ is the associated labeling, 
the learning task consists in finding model parameters $\mathbf{w}$ that
achieve low empirical loss subject to some regularization. This is usually formulated as a quadratic program (QP) with
soft margin constraints~\cite{Tsochantaridis04}:
%
\begin{eqnarray}
\label{eq:ssvm_primal}
& & \min_{\w,\bm{\xi}\geq\mathbf{0}} \frac{\lambda}{2} \vectornorm{\w}^2 + 
	\sum_{n=1}^N \xi_n 
%\\
%\nonumber
%& & \mbox{s.t. } \forall \; n, Y \in \mathcal{Y}_n: S_\mathbf{w}(Y^n) \geq  S_\mathbf{w}(Y) + \Delta(Y^n,Y) - \xi_n,
\\
\nonumber
& & \mbox{s.t. } \forall \; n: S_\mathbf{w}(Y^n) \geq  \max_{Y \in \mathcal{Y}_n}(S_\mathbf{w}(Y) + \Delta(Y^n,Y)) - \xi_n,
\end{eqnarray}
%
where $\mathcal{Y}_n$ is the set of all possible labelings for example
$n$, the constant $\lambda$ controls the trade-off between margin and
training error, and the \emph{task loss} $\Delta$ measures the closeness of any inferred labeling $Y$ to the ground truth labeling $Y^n$.

{\bf TALK ABOUT DBCFW: description + pseudo-code}

\section{Software package}

The use of the \algname requires users to implement the following functions:
\begin{itemize}
\item The joint feature map $\Psi(X,Y)$ which encodes the input/output pairs.
\item The structured loss function $\Delta(Y_i,Y)$.
\item A maximization oracle which computes the most violating constraint by solving Eq. X.
\item A prediction function that computes $X$. This operation is usually performed with the maximization oracle.
\end{itemize}

TODO: Can we show a simple example like the pystruct paper?

\section{Experiments}

Run experiments on cov and other dataset. Compare to other frameworks...

% Acknowledgements should go at the end, before appendices and references
\acks{...}

Thank for all the students...

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{bibliography}

\end{document}
