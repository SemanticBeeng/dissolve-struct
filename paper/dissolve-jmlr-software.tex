\documentclass[twoside,11pt]{article}


% JMLR Machine Learning Open Source Software papers:
% those are 4 pages plus references
% more details:
% http://www.jmlr.org/mloss/mloss-info.html


% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{bm,color}
\usepackage{mathdots}
\usepackage{subcaption}
\usepackage{amsopn}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\definecolor{darkgreen}{rgb}{0,.4,.2}
\definecolor{darkblue}{rgb}{.1,.2,.6}
\definecolor{brightblue}{rgb}{0,0.6,0.8}
\hypersetup{
  colorlinks=true,
  linkcolor=darkblue,
  citecolor=darkgreen,
  filecolor=darkblue,
  urlcolor=darkblue
}

% Definitions of handy macros can go here

\newcommand{\algname}{\textsc{Dissolve}$\,^{\textsf{\tiny struct}}$\xspace}
\newcommand{\svmstruct}{\textsc{SVM}$\,^{\textsf{\tiny struct}}$\xspace}
\newcommand{\cocoa}{\textsc{CoCoA}\xspace}
\newcommand{\cocoap}{\textsc{CoCoA$\!^{\bf \textbf{\footnotesize+}}$}\xspace}
\newcommand{\spark}{\textsc{Spark}\xspace}
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|} 

\newcommand{\comment}[1]{}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newcommand{\alphav}{\bm{\alpha}}
\newcommand{\xv}{{\bf x}}
\newcommand{\yv}{{\bf y}}

% cocoa stuff
\newcommand{\vc}[2]{#1^{(#2)}}
\newcommand{\vsubset}[2]{#1_{[#2]}}
\newcommand{\Ggk}{\mathcal{G}^{\sigma'}_k\hspace{-0.08em}}
\newcommand{\aggpar}{\gamma}

\DeclareMathOperator*{\E}{\rm E}
\newcommand{\simplex}{\Delta}

% general math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\kroneckerdelta}[2]{\delta_{#2}(#1)}
\newcommand{\realnumbers}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\transpose}{\mathsf{T}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand\SetOf[2]{\left\{#1\,\vphantom{#2}\right.\left|\vphantom{#1}\,#2\right\}}

%vectors
\newcommand{\id}{\mathbf{I}} % big i for identity
\newcommand{\ind}{\mathbf{1}} % indicator vectors
\newcommand{\0}{\mathbf{0}} % the origin
\newcommand{\unit}{\mathbf{e}} % unit basis vectors
\newcommand{\one}{\mathbf{1}} % all one vector

\newcommand{\bv}{\bm{b}}

% structured output prediction
\newcommand{\inputvar}{x}
\newcommand{\inputvarv}{\bm{\inputvar}}
\newcommand{\inputdomain}{\mathcal{X}}
\newcommand{\outputvar}{y}
\newcommand{\outputvarv}{\bm{\outputvar}}
\newcommand{\outputdomain}{\mathcal{Y}}
%\newcommand{\hiddenvar}{z}
%\newcommand{\hiddenvarv}{\bm{\hiddenvar}}
%\newcommand{\hiddendomain}{\mathcal{Z}}
\newcommand{\inputoutputdomain}{\mathcal{H}}
\newcommand{\featuremap}{\phi}
\newcommand{\featuremapv}{\bm{\featuremap}}
\newcommand{\featuremapdiff}{\psi}
\newcommand{\featuremapdiffv}{\bm{\featuremapdiff}}
\newcommand{\errorterm}{L}
\newcommand{\weight}{w}
\newcommand{\weightv}{\bm{\weight}}
\newcommand{\wv}{{\bm{\weight}}}
\newcommand{\dualvar}{\alpha}
\newcommand{\dualvarv}{\bm{\alpha}}
\newcommand{\kernelmatrix}{K}
\newcommand{\kernelmatrixv}{\bm{\kernelmatrix}}
\newcommand{\Lmax}{L_{\text{\tiny{max}}}}

% Frank-Wolfe
\newcommand{\sv}{\bm{s}}
\newcommand{\x}{{\bm{x}}} %not to be confused with SVM x
\newcommand{\y}{{\bm{y}}} %not to be confused with SVM y
\newcommand{\e}{{\bm{e}}} % appears in proof of linear convergence...
\newcommand{\stepsize}{\gamma}
\newcommand{\domain}{\mathcal{D}} %TODO: be careful!
%\newcommand{\CfTotal}{C_f^{\text{\tiny{prod}}}}
\newcommand{\CfTotal}{C_{\hspace{-0.1em}f}^\otimes}
\newcommand{\Cf}{C_{\hspace{-0.08em}f}}
\DeclareMathOperator{\approxLin}{\textsc{ApproxLinearMin}}
\DeclareMathOperator{\exactLin}{\textsc{LinearMin}}
\newcommand{\approxArgMin}{\operatornamewithlimits{[approx]\argmin}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\vv}{{\bm{v}}}
\newcommand{\ww}{{\bm{w}}}
\newcommand{\zz}{{\bm{z}}}
\newcommand{\mapprox}{\nu} % multiplicative approximation quality
\newcommand{\addFactor}{\tilde{\gamma}} % constant appearing in the additive approximatoin quality

\newcommand{\note}[1]{{\color{red}#1}}
\newcommand{\ignore}[1]{}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2015}{1-??}{1/15}{1/15}{Tribhuvanesh Orekondy, Martin Jaggi and Aurelien Lucchi}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Orekondy, Jaggi and Lucchi}
\firstpageno{1}

\begin{document}

\title{\algname -- A Library for Distributed Structured Prediction}

\author{%
       \name Tribhuvanesh Orekondy \email torekond@student.ethz.ch  \\
       \addr ETH Zurich
       \AND
       \name Martin Jaggi \email jaggi@inf.ethz.ch \\
       \addr ETH Zurich
       \AND
       \name Aurelien Lucchi \email aurelien.lucchi@inf.ethz.ch  \\
       \addr ETH Zurich
       }
       

\editor{t.b.d.}

\maketitle

\input{00_abstract}
\input{01_introduction}
\input{02_formulation}
\input{03_software}
\input{04_experiments}
\input{05_conclusion}


%
\section{Related Work}
For the different squared hinge loss objective: Multi-core Structural SVM Training \citep{Chang:2013ti}, as well as the distributed setting \citep{Lee:2015wqa}.

Distributed SGD for structured prediction has been studied in \citep{McDonald:2010ub} for the case of the structured perceptron.




% Acknowledgements should go at the end, before appendices and references
\acks{We would like to thank Jan Deriu, Bettina Messmer, Thijs Vogels and Ruben Wolff for contributing to the code and discussions to improve readability.}


% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{references}

\end{document}
